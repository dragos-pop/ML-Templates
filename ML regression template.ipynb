{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML regression template\n",
    "\n",
    "- metric: RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#ensure that plots are displayed inside the notebook\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u> Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(filename, target, random_s, proportion):\n",
    "    # parse the data in a dataftame\n",
    "    df_ = pd.read_csv(filename)\n",
    "    \n",
    "    # performe one-hot encoding to convert categorical variables into dummy variables (ensure features such as name, id are removed before)\n",
    "    df = pd.get_dummies(df_)\n",
    "    names = df.columns\n",
    "    features = list(names)\n",
    "    features.remove(target)\n",
    "\n",
    "    # standardize the data\n",
    "#     scaler = StandardScaler()\n",
    "#     scaled_df = scaler.fit_transform(df)\n",
    "#     scaled_df = pd.DataFrame(scaled_df, columns=names)\n",
    "#   \n",
    "#     # set the target and explanatory variables\n",
    "#     y = scaled_df[target]\n",
    "#     X = scaled_df[features]\n",
    "    \n",
    "    \n",
    "    # set the target and explanatory variables\n",
    "    y = df[target]\n",
    "    X = df[features]\n",
    "    \n",
    "    # split the data in train and test set\n",
    "    r = random_s  # controls how the data are split in train and test sets\n",
    "    proportion_test = proportion  # proportion of data that is sampled as test set\n",
    "    \n",
    "    return train_test_split(X, y, test_size=proportion_test, random_state=r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'data.csv'\n",
    "tar = 'target'\n",
    "ran_state = 1\n",
    "prop = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u> Prediction models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess(file, tar, ran_state, prop)\n",
    "features = list(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates the Linear Regression model\n",
    "lm = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit the train data to the model and print the R^2 of train and test data\n",
    "lm.fit(X_train,y_train)\n",
    "print('Train score (R^2):', lm.score(X_train, y_train))\n",
    "print('Test score (R^2):', lm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute RMSE\n",
    "y_pred = lm.predict(X_test) \n",
    "rmse1 = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print('Root mean squared error:', rmse1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the real values against the predicted values on the test set in a scatter plot\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel(\"Real value\")\n",
    "plt.ylabel(\"Predicted value\")\n",
    "plt.title(\"Linear Regression\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot the real values against the predicted values on the test set in a line plot\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(np.array(y_test), color='red')\n",
    "plt.plot(y_pred, color='blue')\n",
    "plt.xlabel(\"Observations\")\n",
    "plt.ylabel(\"Predicted value\")\n",
    "plt.legend(labels = ['real','predicted'])\n",
    "plt.title('Linear Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute feature importance\n",
    "for feature, importance in zip(features, lm.coef_):\n",
    "    print('Feature: ', feature, '\\t','Importance: ', importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to make prediction\n",
    "d =  # dataframe with explanatory features\n",
    "# e.g. d = X_test.tail(3)\n",
    "# display(d)\n",
    "predicted_values = lm.predict(d)\n",
    "print(\"The predictions are: \", predicted_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) Linear Regression with Backward Elimination (feature selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess(file, tar, ran_state, prop)\n",
    "features = list(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# select the best features with Backward Elimination\n",
    "X_1 = sm.add_constant(X_train) # add dummy feature\n",
    "model = sm.OLS(y_train,X_1).fit() # fit the linear model\n",
    "cols = list(X_train.columns)\n",
    "pmax = 1\n",
    "while (len(cols)>0):\n",
    "    p= []\n",
    "    X_1 = X_train[cols]\n",
    "    X_1 = sm.add_constant(X_1)\n",
    "    model = sm.OLS(y_train,X_1).fit()\n",
    "    p = pd.Series(model.pvalues,index = cols)      \n",
    "    pmax = max(p)\n",
    "    feature_with_p_max = p.idxmax()\n",
    "    if(pmax>0.05):\n",
    "        cols.remove(feature_with_p_max)\n",
    "    else:\n",
    "        break\n",
    "selected_features_BE = cols\n",
    "# print(selected_features_BE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates the Linear Regression model\n",
    "lm2 = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the train data to the model and print the R^2 of train and test data\n",
    "lm2.fit(X_train[selected_features_BE],y_train)\n",
    "print('Train score (R^2):', lm2.score(X_train[selected_features_BE], y_train))\n",
    "print('Test score (R^2):', lm2.score(X_test[selected_features_BE], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute RMSE\n",
    "y_pred = lm2.predict(X_test[selected_features_BE]) \n",
    "rmse2 = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print('Root mean squared error:', rmse2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the real values against the predicted values on the test set in a scatter plot\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel(\"Real value\")\n",
    "plt.ylabel(\"Predicted value\")\n",
    "plt.title(\"Linear Regression with Backward Elimination\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the real values against the predicted values on the test set in a line plot\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(np.array(y_test), color='red')\n",
    "plt.plot(y_pred, color='blue')\n",
    "plt.xlabel(\"Observations\")\n",
    "plt.ylabel(\"Predicted value\")\n",
    "plt.legend(labels = ['real','predicted'])\n",
    "plt.title('Linear Regression with Backward Elimination')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute feature importance\n",
    "for feature, importance in zip(selected_features_BE, lm2.coef_):\n",
    "    print('Feature: ', feature, '\\t','Importance: ', importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to make prediction\n",
    "d =  # dataframe with Backward Elimination selected features\n",
    "# e.g. d = X_test[selected_features_BE].tail(3)\n",
    "# display(d)\n",
    "predicted_values = lm2.predict(d)\n",
    "print(\"The predictions are: \", predicted_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess(file, tar, ran_state, prop)\n",
    "features = list(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Random Forest model with 1000 trees in the forest\n",
    "rf = RandomForestRegressor(n_estimators = 1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the train data to the model and print the R^2 of train and test data\n",
    "rf.fit(X_train, y_train)\n",
    "print('Train score (R^2):', rf.score(X_train, y_train))\n",
    "print('Test score (R^2):', rf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute RMSE\n",
    "y_pred = rf.predict(X_test) \n",
    "rmse3 = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print('Root mean squared error:', rmse3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the real values against the predicted values on the test set in a scatter plot\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel(\"Real value\")\n",
    "plt.ylabel(\"Predicted value\")\n",
    "plt.title(\"Random Forest\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot the real values against the predicted values on the test set in a line plot\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(np.array(y_test), color='red')\n",
    "plt.plot(y_pred, color='blue')\n",
    "plt.xlabel(\"Observations\")\n",
    "plt.ylabel(\"Predicted value\")\n",
    "plt.legend(labels = ['real','predicted'])\n",
    "plt.title('Random Forest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute feature importance\n",
    "for feature, importance in zip(features, rf.feature_importances_):\n",
    "    print('Feature: ', feature, '\\t','Importance: ', importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to make prediction\n",
    "d =  # dataframe with explanatory features\n",
    "# e.g. d = X_test.tail(3)\n",
    "# display(d)\n",
    "predicted_values = rf.predict(d)\n",
    "print(\"The predictions are: \", predicted_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Random forest with Random Search (hyperparameters tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess(file, tar, ran_state, prop)\n",
    "features = list(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning hyperparameters by randomly sampling from given parameters\n",
    "\n",
    "# define values for hyperparameters\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 200)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "# create the Random Forest model \n",
    "rf2 = RandomForestRegressor()\n",
    "# Random Search of parameters using 5-fold Cross Validation\n",
    "rf_random = RandomizedSearchCV(estimator = rf2, param_distributions = random_grid, scoring='neg_root_mean_squared_error', n_iter = 10, cv = 5, random_state = 42, n_jobs = -1)\n",
    "# fit the train data to the model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Random Forest model with the best hyperparameters after Random Search\n",
    "d = rf_random.best_params_\n",
    "rf2 = RandomForestRegressor(n_estimators = d['n_estimators'], min_samples_split = d['min_samples_split'], min_samples_leaf = d['min_samples_leaf'], max_features = d['max_features'], max_depth = d['max_depth'], bootstrap = d['bootstrap'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the train data to the model and print the R^2 of train and test data\n",
    "rf2.fit(X_train,y_train)\n",
    "print('Train score (R^2):', rf2.score(X_train, y_train))\n",
    "print('Test score (R^2):', rf2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute RMSE\n",
    "y_pred = rf2.predict(X_test) \n",
    "rmse4 = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print('Root mean squared error:', rmse4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the real values against the predicted values on the test set in a scatter plot\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel(\"Real value\")\n",
    "plt.ylabel(\"Predicted value\")\n",
    "plt.title(\"Random Forest with Random Search\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the real values against the predicted values on the test set in a line plot\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(np.array(y_test), color='red')\n",
    "plt.plot(y_pred, color='blue')\n",
    "plt.xlabel(\"Observations\")\n",
    "plt.ylabel(\"Predicted value\")\n",
    "plt.legend(labels = ['real','predicted'])\n",
    "plt.title('Random Forest with Random Search')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute feature importance\n",
    "for feature, importance in zip(features, rf2.feature_importances_):\n",
    "    print('Feature: ', feature, '\\t','Importance: ', importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to make prediction\n",
    "d =  # dataframe with explanatory features\n",
    "# e.g. d = X_test.tail(3)\n",
    "# display(d)\n",
    "predicted_values = rf2.predict(d)\n",
    "print(\"The predictions are: \", predicted_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess(file, tar, ran_state, prop)\n",
    "features = list(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Gradient Boosting model with the following hyperparameters \n",
    "params = {\n",
    "    'learning_rate': 0.05,\n",
    "    \"num_leaves\": 1000,  \n",
    "    \"n_estimators\": 1000\n",
    "}\n",
    "gbm = lgb.LGBMRegressor(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the train data to the model and print the R^2 of train and test data\n",
    "gbm.fit(X_train, y_train);\n",
    "print('Train score (R^2):', gbm.score(X_train, y_train))\n",
    "print('Test score (R^2):', gbm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute RMSE\n",
    "y_pred = gbm.predict(X_test, num_iterations = 1000)\n",
    "rmse5 = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print('Root mean squared error:', rmse5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the real values against the predicted values on the test set in a scatter plot\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel(\"Real value\")\n",
    "plt.ylabel(\"Predicted value\")\n",
    "plt.title(\"Gradient Boosting\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the real values against the predicted values on the test set in a line plot\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(np.array(y_test), color='red')\n",
    "plt.plot(y_pred, color='blue')\n",
    "plt.xlabel(\"Observations\")\n",
    "plt.ylabel(\"Predicted value\")\n",
    "plt.legend(labels = ['real','predicted'])\n",
    "plt.title('Gradient Boosting')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute feature importance\n",
    "for feature, importance in zip(features, gbm.feature_importances_):\n",
    "    print('Feature: ', feature, '\\t','Importance: ', importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to make prediction\n",
    "d =  # dataframe with explanatory features\n",
    "# e.g. d = X_test.tail(3)\n",
    "# display(d)\n",
    "predicted_values = gbm.predict(d)\n",
    "print(\"The predictions are: \", predicted_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) Gradient Boosting with Random Search (hyperparameters tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess(file, tar, ran_state, prop)\n",
    "features = list(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning hyperparameters by randomly sampling from given parameters.\n",
    "\n",
    "# define values for hyperparameters\n",
    "learning_rate = [x for x in np.linspace(0.01, 1, num = 100)]\n",
    "num_leaves = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 200)]\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 200)]\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 100)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'num_leaves': num_leaves,\n",
    "               'max_depth': max_depth,\n",
    "               'learning_rate': learning_rate}\n",
    "\n",
    "# create the Gradient Boosting model\n",
    "gb = lgb.LGBMRegressor()\n",
    "# Random Search of parameters using 5-fold Cross Validation\n",
    "gb_random = RandomizedSearchCV(estimator = gb, param_distributions = random_grid, scoring='neg_root_mean_squared_error', n_iter = 10, cv = 5, random_state = 42, n_jobs = -1);\n",
    "# fit the train data to the model\n",
    "gb_random.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Gradient Boosting model with the best hyperparameters after Random Search\n",
    "d = gb_random.best_params_\n",
    "gbm2 = lgb.LGBMRegressor(num_leaves = d['num_leaves'],n_estimators = d['n_estimators'],max_depth = d['max_depth'], learning_rate = d['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the train data to the model and print the R^2 of train and test data\n",
    "gbm2.fit(X_train,y_train);\n",
    "print('train score (R^2):', gbm2.score(X_train, y_train))\n",
    "print('test score (R^2):', gbm2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute RMSE\n",
    "y_pred = gbm2.predict(X_test)\n",
    "rmse6 = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print('Root mean squared error:', rmse6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the real values against the predicted values on the test set in a scatter plot\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel(\"Real value\")\n",
    "plt.ylabel(\"Predicted value\")\n",
    "plt.title(\"Gradient Boosting with Random Search\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the real values against the predicted values on the test set in a line plot\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(np.array(y_test), color='red')\n",
    "plt.plot(y_pred, color='blue')\n",
    "plt.xlabel(\"Observations\")\n",
    "plt.ylabel(\"Predicted value\")\n",
    "plt.legend(labels = ['real','predicted'])\n",
    "plt.title('Gradient Boosting with Random Search')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute feature importance\n",
    "for feature, importance in zip(features, gbm2.feature_importances_):\n",
    "    print('Feature: ', feature, '\\t','Importance: ', importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to make prediction\n",
    "d =  # dataframe with explanatory features\n",
    "# e.g. d = X_test.tail(3)\n",
    "# display(d)\n",
    "predicted_values = gbm2.predict(d)\n",
    "print(\"The predictions are: \", predicted_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1) Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess(file, tar, ran_state, prop)\n",
    "features = list(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "# create the Neural Network \n",
    "ann = Sequential()\n",
    "# add the input layer and the first hidden layer\n",
    "ann.add(Dense(32, activation = 'relu', input_dim = len(features)))\n",
    "# add the second hidden layer\n",
    "ann.add(Dense(units = 32, activation = 'relu'))\n",
    "# add the third hidden layer\n",
    "ann.add(Dense(units = 32, activation = 'relu'))\n",
    "# add the output layer\n",
    "ann.add(Dense(units = 1))\n",
    "# compile the ANN\n",
    "ann.compile(optimizer = 'adam', loss = 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the train data to the model\n",
    "ann.fit(X_train, y_train, batch_size = 10, epochs = 100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute RMSE\n",
    "y_pred = ann.predict(X_test);\n",
    "rmse7 = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print('Root mean squared error:', rmse7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the real values against the predicted values on the test set in a scatter plot\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel(\"Real value\")\n",
    "plt.ylabel(\"Predicted value\")\n",
    "plt.title(\"Neural Network\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the real values against the predicted values on the test set in a line plot\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(np.array(y_test), color='red')\n",
    "plt.plot(y_pred, color='blue')\n",
    "plt.xlabel(\"Observations\")\n",
    "plt.ylabel(\"Predicted value\")\n",
    "plt.legend(labels = ['real','predicted'])\n",
    "plt.title('Neural Network')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to make prediction\n",
    "d =  # dataframe with explanatory features\n",
    "# e.g. d = X_test.tail(3)\n",
    "# display(d)\n",
    "predicted_values = ann.predict(d)\n",
    "print(\"The predictions are: \", predicted_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2) Neural Network with Grid Search (hyperparameters tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess(file, tar, ran_state, prop)\n",
    "features = list(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tuning hyperparameters by picking all combinations of the given parameters\n",
    "\n",
    "def create_model():\n",
    "    # create the Neural Network \n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=len(features), activation='relu'))\n",
    "    model.add(Dense(units = 32, activation = 'relu'))\n",
    "    model.add(Dense(units = 32, activation = 'relu'))\n",
    "    model.add(Dense(1))\n",
    "    # compile the ANN\n",
    "    model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# define values for hyperparameters\n",
    "batch_size = [10, 20, 40, 60]\n",
    "epochs = [10, 50, 100]\n",
    "# create the grid\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "# create the Neural Network\n",
    "nn = KerasRegressor(build_fn=create_model)\n",
    "# Grid Search of parameters using 5-fold Cross Validation\n",
    "nn_grid = GridSearchCV(estimator=nn, param_grid=param_grid, n_jobs=-1, cv=5, scoring='neg_root_mean_squared_error')\n",
    "# fit the train data to the model\n",
    "grid_result = nn_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Neural Network model with the best hyperparameters after Grid Search\n",
    "d = grid_result.best_params_\n",
    "# create the Neural Network \n",
    "ann2 = Sequential()\n",
    "# add the input layer and the first hidden layer\n",
    "ann2.add(Dense(units = 32, activation = 'relu', input_dim = len(features)))\n",
    "# add the second hidden layer\n",
    "ann2.add(Dense(units = 32, activation = 'relu'))\n",
    "# add the third hidden layer\n",
    "ann2.add(Dense(units = 32, activation = 'relu'))\n",
    "# add the output layer\n",
    "ann2.add(Dense(units = 1))\n",
    "# compile the ANN\n",
    "ann2.compile(optimizer = 'adam', loss = 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the train data to the model\n",
    "ann2.fit(X_train, y_train, batch_size = d['batch_size'], epochs =d['epochs']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute RMSE\n",
    "y_pred = ann2.predict(X_test);\n",
    "rmse8 = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print('Root mean squared error:', rmse8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the real values against the predicted values on the test set in a scatter plot\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel(\"Real value\")\n",
    "plt.ylabel(\"Predicted value\")\n",
    "plt.title(\"Neural Network with Grid Search\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the real values against the predicted values on the test set in a line plot\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(np.array(y_test), color='red')\n",
    "plt.plot(y_pred, color='blue')\n",
    "plt.xlabel(\"Observations\")\n",
    "plt.ylabel(\"Predicted value\")\n",
    "plt.legend(labels = ['real','predicted'])\n",
    "plt.title('Neural Network with Grid Search')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to make prediction\n",
    "d =  # dataframe with explanatory features\n",
    "# e.g. d = X_test.tail(3)\n",
    "# display(d)\n",
    "predicted_values = ann2.predict(d)\n",
    "print(\"The predictions are: \", predicted_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u> Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Linear Regression:                               ', rmse1)\n",
    "print('Linear Regression with Backward Elimination:     ', rmse2)\n",
    "print()\n",
    "print('Random Forest:                                   ', rmse3)\n",
    "print('Random Forest with Random Search:                ', rmse4)\n",
    "print()\n",
    "print('Gradient Boosting:                               ', rmse5)\n",
    "print('Gradient Boosting with Random Search:            ', rmse6)\n",
    "print()\n",
    "print('Neural Network:                                  ', rmse7)\n",
    "print('Neural Network with Grid Search:                 ', rmse8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
