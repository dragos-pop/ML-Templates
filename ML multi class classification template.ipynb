{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML multi class classification template\n",
    "\n",
    "- metric: F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#ensure that plots are displayed inside the notebook\n",
    "import math\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u> Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(filename, target, random_s, proportion):\n",
    "    # parse the data in a dataftame\n",
    "    df = pd.read_csv(filename)\n",
    "    df = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))    \n",
    "    names = df.columns\n",
    "    features = list(names)\n",
    "    features.remove(target)\n",
    "    \n",
    "    # standardize the data\n",
    "#     scaler = StandardScaler()\n",
    "#     scaled_df = scaler.fit_transform(df)\n",
    "#     scaled_df = pd.DataFrame(scaled_df, columns=names)\n",
    "#   \n",
    "#     # set the target and explanatory variables\n",
    "#     y = scaled_df[target] \n",
    "#     X = scaled_df[features]\n",
    "    \n",
    "    \n",
    "    # set the target and explanatory variables\n",
    "    y = df[target]\n",
    "    X = df[features]\n",
    "    \n",
    "    # split the data in train and test set\n",
    "    r = random_s  # controls how the data are split in train and test sets\n",
    "    proportion_test = proportion  # proportion of data that is sampled as test set\n",
    "    \n",
    "    return train_test_split(X, y, test_size=proportion_test, random_state=r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'data.csv'\n",
    "tar = 'target'\n",
    "ran_state = 1\n",
    "prop = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u> Classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess(file, tar, ran_state, prop)\n",
    "features = list(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Logistic Regression model\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit the train data to the model\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute F1 score\n",
    "y_pred = lr.predict(X_test) \n",
    "f1_1 = metrics.f1_score(y_test, y_pred, average = 'weighted')\n",
    "print(\"F1 score:\", f1_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(lr, X_test, y_test, cmap='Reds')  \n",
    "plt.title(\"Confusion Matrix: Logistic Regression\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute feature importance\n",
    "for feature, importance in zip(features, list(lr.coef_[0])):\n",
    "    print('Feature: ', feature, '\\t','Importance: ', importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to make classification\n",
    "d =  # dataframe with explanatory features\n",
    "# e.g. d = X_test.tail(3)\n",
    "# display(d)\n",
    "classified_labels = lr.predict(d)\n",
    "print(\"The classifications are: \", classified_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) Logistic Regression with Recursive Feature Elimination (feature selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess(file, tar, ran_state, prop)\n",
    "features = list(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# select the best n_features_to_select with Recursive Feature Elimination\n",
    "rfe = RFE(estimator=LogisticRegression(), n_features_to_select=5)\n",
    "lr_ = rfe.fit(X_train, y_train)\n",
    "selected_features_RFE = list(X_train.columns[list(lr_.support_)])\n",
    "# print(selected_features_RFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Logistic Regression model\n",
    "lr2 = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the train data to the model\n",
    "lr2.fit(X_train[selected_features_RFE],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute F1 score\n",
    "y_pred = lr2.predict(X_test[selected_features_RFE]) \n",
    "f1_2 = metrics.f1_score(y_test, y_pred, average = 'weighted')\n",
    "print(\"F1 score:\", f1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(lr2, X_test[selected_features_RFE], y_test, cmap='Reds')  \n",
    "plt.title(\"Confusion Matrix: Logistic Regression with Recursive Feature Elimination\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute feature importance\n",
    "for feature, importance in zip(selected_features_RFE, list(lr2.coef_[0])):\n",
    "    print('Feature: ', feature, '\\t','Importance: ', importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to make classification\n",
    "d =  # dataframe with Recursive Feature Elimination selected features\n",
    "# e.g. d = X_test[selected_features_RFE].tail(3)\n",
    "# display(d)\n",
    "classified_labels = lr2.predict(d)\n",
    "print(\"The classifications are: \", classified_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess(file, tar, ran_state, prop)\n",
    "features = list(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Random Forest model with 1000 trees in the forest\n",
    "rf = RandomForestClassifier(n_estimators = 1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the train data to the model\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute F1 score\n",
    "y_pred = rf.predict(X_test) \n",
    "f1_3 = metrics.f1_score(y_test, y_pred, average = 'weighted')\n",
    "print(\"F1 score:\", f1_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(rf, X_test, y_test, cmap='Reds')  \n",
    "plt.title(\"Confusion Matrix: Random Forest\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute feature importance\n",
    "for feature, importance in zip(features, rf.feature_importances_):\n",
    "    print('Feature: ', feature, '\\t','Importance: ', importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to make classification\n",
    "d =  # dataframe with explanatory features\n",
    "# e.g. d = X_test.tail(3)\n",
    "# display(d)\n",
    "classified_labels = rf.predict(d)\n",
    "print(\"The classifications are: \", classified_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Random Forest with Random Search (hyperparameters tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess(file, tar, ran_state, prop)\n",
    "features = list(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning hyperparameters by randomly sampling from given parameters\n",
    "\n",
    "# define values for hyperparameters\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 200)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "# create the Random Forest model \n",
    "rf_ = RandomForestClassifier()\n",
    "# Random Search of parameters using 5-fold Cross Validation\n",
    "scorer = metrics.make_scorer(metrics.f1_score, average = 'weighted')\n",
    "rf_random = RandomizedSearchCV(estimator = rf_, param_distributions = random_grid, scoring=scorer, n_iter = 10, cv = 5, random_state = 42, n_jobs = -1)\n",
    "# fit the train data to the model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Random Forest model with the best hyperparameters after Random Search\n",
    "d = rf_random.best_params_\n",
    "rf2 = RandomForestClassifier(n_estimators = d['n_estimators'], min_samples_split = d['min_samples_split'], min_samples_leaf = d['min_samples_leaf'], max_features = d['max_features'], max_depth = d['max_depth'], bootstrap = d['bootstrap'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the train data to the model\n",
    "rf2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute F1 score\n",
    "y_pred = rf2.predict(X_test) \n",
    "f1_4 = metrics.f1_score(y_test, y_pred, average = 'weighted')\n",
    "print(\"F1 score:\", f1_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(rf2, X_test, y_test, cmap='Reds')  \n",
    "plt.title(\"Confusion Matrix: Random Forest with Random Search\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute feature importance\n",
    "for feature, importance in zip(features, rf2.feature_importances_):\n",
    "    print('Feature: ', feature, '\\t','Importance: ', importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to make classification\n",
    "d =  # dataframe with explanatory features\n",
    "# e.g. d = X_test.tail(3)\n",
    "# display(d)\n",
    "classified_labels = rf2.predict(d)\n",
    "print(\"The classifications are: \", classified_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess(file, tar, ran_state, prop)\n",
    "features = list(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Gradient Boositing model with the following hyperparameters \n",
    "params = {\n",
    "    'learning_rate': 0.05,\n",
    "    \"num_leaves\": 1000,  \n",
    "    \"n_estimators\": 1000\n",
    "}\n",
    "gbm = lgb.LGBMClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the train data to the model\n",
    "gbm.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute F1 score\n",
    "y_pred = gbm.predict(X_test, num_iterations = 1000)\n",
    "f1_5 = metrics.f1_score(y_test, y_pred, average = 'weighted')\n",
    "print(\"F1 score:\", f1_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(gbm, X_test, y_test, cmap='Reds')  \n",
    "plt.title(\"Confusion Matrix: Gradient Boosting\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute feature importance\n",
    "for feature, importance in zip(features, gbm.feature_importances_):\n",
    "    print('Feature: ', feature, '\\t','Importance: ', importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to make classification\n",
    "d =  # dataframe with explanatory features\n",
    "# e.g. d = X_test.tail(3)\n",
    "# display(d)\n",
    "classified_labels = gbm.predict(d)\n",
    "print(\"The classifications are: \", classified_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) Gradient Boosting with Random Search (hyperparameters tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess(file, tar, ran_state, prop)\n",
    "features = list(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning hyperparameters by randomly sampling from given parameters.\n",
    "\n",
    "# define values for hyperparameters\n",
    "learning_rate = [x for x in np.linspace(0.01, 1, num = 100)]\n",
    "num_leaves = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 200)]\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 200)]\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 100)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'num_leaves': num_leaves,\n",
    "               'max_depth': max_depth,\n",
    "               'learning_rate': learning_rate}\n",
    "\n",
    "# create the Gradient Boosting model\n",
    "gb = lgb.LGBMClassifier()\n",
    "# Random Search of parameters using 5-fold Cross Validation\n",
    "scorer = metrics.make_scorer(metrics.f1_score, average = 'weighted')\n",
    "gb_random = RandomizedSearchCV(estimator = gb, param_distributions = random_grid, scoring=scorer, n_iter = 10, cv = 5, random_state = 42, n_jobs = -1);\n",
    "# fit the train data to the model\n",
    "gb_random.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Gradient Boosting model with the best hyperparameters after Random Search\n",
    "d = gb_random.best_params_\n",
    "gbm2 = lgb.LGBMClassifier(num_leaves = d['num_leaves'],n_estimators = d['n_estimators'],max_depth = d['max_depth'], learning_rate = d['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the train data to the model\n",
    "gbm2.fit(X_train,y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute F1 score\n",
    "y_pred = gbm2.predict(X_test)\n",
    "f1_6 = metrics.f1_score(y_test, y_pred, average = 'weighted')\n",
    "print(\"F1 score:\", f1_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(gbm2, X_test, y_test, cmap='Reds')  \n",
    "plt.title(\"Confusion Matrix: Gradient Boosting with Random Search\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute feature importance\n",
    "for feature, importance in zip(features, gbm2.feature_importances_):\n",
    "    print('Feature: ', feature, '\\t','Importance: ', importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to make classification\n",
    "d =  # dataframe with explanatory features\n",
    "# e.g. d = X_test.tail(3)\n",
    "# display(d)\n",
    "classified_labels = gbm2.predict(d)\n",
    "print(\"The classifications are: \", classified_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1) K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess(file, tar, ran_state, prop)\n",
    "features = list(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the K-Nearest Neighbors model with 10 neighbors\n",
    "knn = KNeighborsClassifier(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the train data to the model\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute F1 score\n",
    "y_pred = knn.predict(X_test)\n",
    "f1_7 = metrics.f1_score(y_test, y_pred, average = 'weighted')\n",
    "print(\"F1 score:\", f1_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(knn, X_test, y_test, cmap='Reds')  \n",
    "plt.title(\"Confusion Matrix: K-Nearest Neighbors\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to make classification\n",
    "d =  # dataframe with explanatory features\n",
    "# e.g. d = X_test.tail(3)\n",
    "# display(d)\n",
    "classified_labels = knn.predict(d)\n",
    "print(\"The classifications are: \", classified_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2) K-Nearest Neighbors with Grid Search (hyperparameters tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess(file, tar, ran_state, prop)\n",
    "features = list(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning hyperparameters by randomly sampling from given parameters.\n",
    "\n",
    "# define values for hyperparameters\n",
    "n_neighbors = [int(x) for x in np.linspace(2, 20, 19)]\n",
    "# create the  grid\n",
    "param_grid = {'n_neighbors': n_neighbors}\n",
    "\n",
    "# create the K-Nearest Neighbors model\n",
    "knn_ = KNeighborsClassifier()\n",
    "# Grid Search of parameters using 5-fold Cross Validation\n",
    "scorer = metrics.make_scorer(metrics.f1_score, average = 'weighted')\n",
    "knn_grid = GridSearchCV(estimator=knn_, param_grid=param_grid, n_jobs=-1, cv=5, scoring=scorer)\n",
    "# fit the train data to the model\n",
    "grid_result = knn_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the K-Nearest Neighbors model with the best hyperparameters after Random Search\n",
    "d = grid_result.best_params_\n",
    "knn2 = KNeighborsClassifier(n_neighbors = d['n_neighbors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the train data to the model\n",
    "knn2.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute F1 score\n",
    "y_pred = knn2.predict(X_test);\n",
    "f1_8 = metrics.f1_score(y_test, y_pred, average = 'weighted')\n",
    "print(\"F1 score:\", f1_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(knn2, X_test, y_test, cmap='Reds')  \n",
    "plt.title(\"Confusion Matrix: K-Nearest Neighbors with Grid Search\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to make classification\n",
    "d =  # dataframe with explanatory features\n",
    "# e.g. d = X_test.tail(3)\n",
    "# display(d)\n",
    "classified_labels = knn2.predict(d)\n",
    "print(\"The classifications are: \", classified_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u> Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Logistic Regression:                             ', f1_1)\n",
    "print('Logistic Regression with RFE:                    ', f1_2)\n",
    "print()\n",
    "print('Random Forest:                                   ', f1_3)\n",
    "print('Random Forest with Random Search:                ', f1_4)\n",
    "print()\n",
    "print('Gradient Boosting:                               ', f1_5)\n",
    "print('Gradient Boosting with Random Search:            ', f1_6)\n",
    "print()\n",
    "print('K-Nearest Neighbors:                             ', f1_7)\n",
    "print('K-Nearest Neighbors with Grid Search:            ', f1_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
